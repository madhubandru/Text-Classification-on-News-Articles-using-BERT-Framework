# Text-Classification-on-News-Articles-using-BERT-Framework

### Team Members
* Madhu Bandru - mb4236@drexel.edu
* Spandana Bendi - sb4262@drexel.edu
* Vuthej Krishna Reddy Verama Reddy - vv334@drexel.edu


### Files included

#### Code Files

As implemented three different succesful approaches for text classification, we have maintained a single notebook for each approach. In approach experimented is not successfully completed so, we dint include the notebook for same.

1.	Approach-1: Baseline Model – Simple Sequential Neural Network.
2.	Approach-2: BERT Model with Keras Embeddings.
3.	Approach-3: BERT model using BERT’s default embedding technique.
4.	Approach-4: BERT model using embeddings from ELMo.

<b>Colab Links</b>
* Approach-1: https://colab.research.google.com/drive/1n_cOeRdQVS2hQDtZA-hPL7Yt3r4GhY4w#scrollTo=fqfaav6ay9Zf
* Approach-2: https://colab.research.google.com/drive/1pGUD1AJ5gCdnUzk_gXI_s0fi4M0v--QK#scrollTo=b2XOSLa3SqBh
* Approach-3: https://colab.research.google.com/drive/1aEXyBsM7l6kCXBs6Uxyz2g1aUSu0OgLv#scrollTo=bW4GkNDLwEei

#### Data Files

We have included the csv files of original data set and also include csv names <b>sample.csv</b> in which added the header and used same file all experiments.

#### Text Preprocessing File

We have maintained a separate py file for text cleaning for removing junk characters, special characters, extra white spaces and convert number to words. We used this <b>preprocess_data.py<b> to clean the article column from the loaded data.

#### Text Preprocessing File
